# GroundedPRM 核心算法与项目动机说明

## 1. 项目动机（Why）

大模型在数学推理任务上常见两个问题：

1. **只看最终答案，不看过程质量**：即使答案对了，中间步骤也可能有逻辑漏洞。
2. **过程监督数据噪声高**：很多 PRM 数据来自启发式或弱标注，步骤标签不稳定，导致奖励模型学到“表面模式”而非真实推理质量。

GroundedPRM 的出发点是：

1. 用**树搜索**生成多样化推理路径，避免单一路径监督偏差。
2. 用**外部可验证信号**（Wolfram Alpha / 数学等价校验）提升步骤标签可信度。
3. 将“步骤正确性 + 最终结果正确性”联合建模，形成更稳健的过程奖励。

## 2. 核心思想（What）

GroundedPRM 可以概括为三层：

1. **Tree-Guided**：用 MCTS 探索多个中间推理分支。
2. **Fidelity-Aware**：对每一步引入外部验证与评审，减少幻觉步骤被误标为正确的概率。
3. **Process Supervision**：将步骤级奖励转为可训练数据，用于微调 PRM。

## 3. MCTS 原理简述

MCTS（Monte Carlo Tree Search）是一种在巨大搜索空间中逐步逼近优解的策略，标准流程包含 4 步：

1. **Selection（选择）**：从根节点出发，依据树策略（如 UCT）选择要扩展的叶/边界节点。
2. **Expansion（扩展）**：在该节点上生成一个或多个新子节点（候选动作）。
3. **Simulation（模拟）**：从新节点向前 rollout，得到一个终局回报估计。
4. **Backpropagation（回传）**：把回报沿路径向上更新，影响后续选择。

通过多轮迭代，搜索会在“高价值分支”上投入更多预算，同时保持一定探索。

## 4. MCTS 在本项目中的具体化（How）

对应实现主要在 `pipeline/MCTS/mcts.py` 与 `pipeline/MCTS/mcts_utils.py`。

### 4.1 节点与状态定义

每个节点是一个中间推理状态 `State`，包含：
- 全局目标 `global_objective`
- 题目条件 `conditions`
- 已执行步骤 `steps`
- 当前步骤目标/答案 `step_objective`, `step_ans`
- 当前步骤对应的 WA 查询与返回 `query`, `wa_history`

这使得每个节点不仅有“文本推理内容”，还有“可验证证据”。

### 4.2 Selection（选择）

项目在选择阶段结合节点价值与访问信息进行分支选择，并按阶段控制偏好：
- 先收集正样本，再补充负样本。
- 通过探索项避免始终走同一条路径。

### 4.3 Expansion（扩展）

对当前状态调用策略模型生成下一步（JSON 形式）：
- `step objective`
- `action`

每个候选步骤形成一个子节点，加入搜索树。

### 4.4 Simulation（模拟）

从扩展节点继续 rollout 若干步：
- 继续生成下一步推理；
- 每一步调用验证模块评估对错，得到步骤分数；
- 到终止时再计算最终答案分数。

### 4.5 Reward 设计

本项目回报由两部分构成：

1. **步骤级回报**：
   - 用 `llm_verify` 综合“步骤目标、上下文条件、LLM 步骤内容、WA 返回”判定 `True/False`；
   - 映射为数值奖励（正/负）。

2. **终局回报**：
   - 从步骤中抽取 `\boxed{}` 结果；
   - 与 ground truth 做数学等价校验（`math_verify`）。

最终在回传时对路径奖励聚合并衰减传播到祖先节点。

### 4.6 终止条件

常见终止信号：
- 步骤文本显式 `<end>`；
- 当前步骤结果与标准答案等价；
- 出现重复答案/无效循环迹象。

### 4.7 输出产物

MCTS 结束后保存两类关键产物：

1. **完整树结构**：用于审计与后续路径抽取。
2. **正负样本路径**：用于过程监督训练数据构建。

## 5. 为什么这种方法有效

相比只监督最终答案，该方法优势在于：

1. **监督更细粒度**：能识别“最后对但中间错”或“中间局部错导致最终错”的情形。
2. **标签更可追溯**：每一步有验证依据和反思文本，训练信号更可解释。
3. **搜索与奖励闭环**：MCTS 生成数据，PRM 学习步骤质量，评测时再用 PRM 引导搜索，形成正循环。

## 6. 项目一句话总结

GroundedPRM 通过“**MCTS 多路径探索 + 外部验证增强 + 步骤级奖励建模**”，把数学推理从“结果导向”升级为“过程可验证导向”，从而提升过程奖励模型的可靠性与泛化表现。

